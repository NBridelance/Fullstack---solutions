{"cells":[{"cell_type":"markdown","metadata":{"id":"2JnnD6qpLGPd"},"source":["# Discovering the Titanic üö¢üö¢\n","\n","The Titanic is a well known ship, but did you know that it is also one of the most popular datasets in Data Science ? Here's the link to the dataset:\n","\n","<a href=\"https://www.kaggle.com/c/titanic/\"> Titanic </a>\n","\n","Machine Learning is of course all about statistical prediction and understanding of data. The objective of this exercise is to predict whether a passenger survived the sinking of the Titanic, based on the information available about that passenger. The part of the code to train the model, make predictions and evaluate its performance has already been coded. You have to complete the upstream part, which will allow you to prepare the dataset before training the model (preprocessing).\n","\n","1. Download the dataset _train.csv_.\n","2. Try to understand what's in this dataset.\n","    1. You will find all the explanations via this link : <a href=\"https://www.kaggle.com/c/titanic/data\"> Titanic Data </a>"]},{"cell_type":"markdown","metadata":{"id":"0LUW5do7LGPi"},"source":["3. Place the file train.csv in the same folder as this notebook and read it."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dxnN7PveLGPi","executionInfo":{"status":"ok","timestamp":1648065503979,"user_tz":-60,"elapsed":1369,"user":{"displayName":"Rodelin Exavier","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkXZv4QBdZ8xXYm1Ryw6nFimLdOxqrShqkK_c=s64","userId":"08855355264744746322"}}},"outputs":[],"source":["# We need pandas library\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[],"id":"D1Uz5TWULGPk","executionInfo":{"status":"error","timestamp":1648065507994,"user_tz":-60,"elapsed":963,"user":{"displayName":"Rodelin Exavier","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkXZv4QBdZ8xXYm1Ryw6nFimLdOxqrShqkK_c=s64","userId":"08855355264744746322"}},"outputId":"1add9e71-a011-4960-d74f-8cb3df9d3f02","colab":{"base_uri":"https://localhost:8080/","height":310}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-7925e1794481>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"]}],"source":["dataset = pd.read_csv('train.csv')"]},{"cell_type":"markdown","metadata":{"id":"zF5UaqzILGPk"},"source":["4. Explore the dataset and determine which columns are useful for prediction and what preprocessing you will do."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"oF_ITwfVLGPk","outputId":"27f36345-2e8f-4d91-e4a2-4388d3d7dafb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows : 891\n","\n","Display of dataset: \n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Basics statistics: \n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>count</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891</td>\n","      <td>891</td>\n","      <td>714.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891</td>\n","      <td>891.000000</td>\n","      <td>204</td>\n","      <td>889</td>\n","    </tr>\n","    <tr>\n","      <td>unique</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>891</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>681</td>\n","      <td>NaN</td>\n","      <td>147</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <td>top</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Kent, Mr. Edward Austin</td>\n","      <td>male</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>347082</td>\n","      <td>NaN</td>\n","      <td>G6</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <td>freq</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>577</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>644</td>\n","    </tr>\n","    <tr>\n","      <td>mean</td>\n","      <td>446.000000</td>\n","      <td>0.383838</td>\n","      <td>2.308642</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>29.699118</td>\n","      <td>0.523008</td>\n","      <td>0.381594</td>\n","      <td>NaN</td>\n","      <td>32.204208</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <td>std</td>\n","      <td>257.353842</td>\n","      <td>0.486592</td>\n","      <td>0.836071</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>14.526497</td>\n","      <td>1.102743</td>\n","      <td>0.806057</td>\n","      <td>NaN</td>\n","      <td>49.693429</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <td>min</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.420000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <td>25%</td>\n","      <td>223.500000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20.125000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","      <td>7.910400</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <td>50%</td>\n","      <td>446.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>28.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","      <td>14.454200</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <td>75%</td>\n","      <td>668.500000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>38.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","      <td>31.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <td>max</td>\n","      <td>891.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>80.000000</td>\n","      <td>8.000000</td>\n","      <td>6.000000</td>\n","      <td>NaN</td>\n","      <td>512.329200</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        PassengerId    Survived      Pclass                     Name   Sex  \\\n","count    891.000000  891.000000  891.000000                      891   891   \n","unique          NaN         NaN         NaN                      891     2   \n","top             NaN         NaN         NaN  Kent, Mr. Edward Austin  male   \n","freq            NaN         NaN         NaN                        1   577   \n","mean     446.000000    0.383838    2.308642                      NaN   NaN   \n","std      257.353842    0.486592    0.836071                      NaN   NaN   \n","min        1.000000    0.000000    1.000000                      NaN   NaN   \n","25%      223.500000    0.000000    2.000000                      NaN   NaN   \n","50%      446.000000    0.000000    3.000000                      NaN   NaN   \n","75%      668.500000    1.000000    3.000000                      NaN   NaN   \n","max      891.000000    1.000000    3.000000                      NaN   NaN   \n","\n","               Age       SibSp       Parch  Ticket        Fare Cabin Embarked  \n","count   714.000000  891.000000  891.000000     891  891.000000   204      889  \n","unique         NaN         NaN         NaN     681         NaN   147        3  \n","top            NaN         NaN         NaN  347082         NaN    G6        S  \n","freq           NaN         NaN         NaN       7         NaN     4      644  \n","mean     29.699118    0.523008    0.381594     NaN   32.204208   NaN      NaN  \n","std      14.526497    1.102743    0.806057     NaN   49.693429   NaN      NaN  \n","min       0.420000    0.000000    0.000000     NaN    0.000000   NaN      NaN  \n","25%      20.125000    0.000000    0.000000     NaN    7.910400   NaN      NaN  \n","50%      28.000000    0.000000    0.000000     NaN   14.454200   NaN      NaN  \n","75%      38.000000    1.000000    0.000000     NaN   31.000000   NaN      NaN  \n","max      80.000000    8.000000    6.000000     NaN  512.329200   NaN      NaN  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Percentage of missing values: \n"]},{"data":{"text/plain":["PassengerId     0.000000\n","Survived        0.000000\n","Pclass          0.000000\n","Name            0.000000\n","Sex             0.000000\n","Age            19.865320\n","SibSp           0.000000\n","Parch           0.000000\n","Ticket          0.000000\n","Fare            0.000000\n","Cabin          77.104377\n","Embarked        0.224467\n","dtype: float64"]},"metadata":{},"output_type":"display_data"}],"source":["# Basic stats\n","print(\"Number of rows : {}\".format(dataset.shape[0]))\n","print()\n","\n","print(\"Display of dataset: \")\n","display(dataset.head())\n","print()\n","\n","print(\"Basics statistics: \")\n","data_desc = dataset.describe(include='all')\n","display(data_desc)\n","print()\n","\n","print(\"Percentage of missing values: \")\n","display(100*dataset.isnull().sum()/dataset.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"iV3ZrHJbLGPl"},"source":["The exploration of the above data makes it possible to know which pre-processing will be necessary:\n","\n","**A. Preprocessing to be planned with pandas**\n","\n","**Unnecessary columns for prediction, to be thrown away** :\n","- _PassengerId_ and _Name_ are passenger identifiers, we won't use them for prediction (these columns don't contain any information)\n","- Ticket_ and Cabin_ have too many different modalities, they might not be very useful and if we had to pass them in OneHotEncoding, they would make the number of columns explode in relation to the number of rows.\n","\n","**Columns with too many missing values, to be discarded** : Cabin\n","\n","\n","**Target variable/target (Y) that we will try to predict, to separate from the others** : Survived\n","\n","**------------**\n","\n","**B. Preprocessings to be planned with scikit-learn**.\n","\n","**Explanatory variables (X)**\n","We need to identify which columns contain categorical variables and which columns contain numerical variables, as they will be treated differently.\n","\n","- Categorical variables : Sex, Embarked\n","- Numerical variables : Class, Age, Bbsp, Parch, Fare.\n","\n","In this dataset, we have both types of variables. It will thus be necessary to plan to create a numeric_transformer (which will call the StandardScaler class) and a categorical_transformer (which will call the OneHotEncoder class). Moreover, as we observe missing values in the _Age_ and _Embarked_ columns, we will have to plan to call the SimpleImputer class to handle the missing values. \n","\n","**Target variable Y**\n","Here, the target variable Y is categorical (survival vs. death) but we notice that it is already encoded in numbers (1 vs. 0). It will therefore not be necessary to go through a label encoding step."]},{"cell_type":"markdown","metadata":{"id":"4b78HL_MLGPm"},"source":["## Preprocessing - pandas part üêºüêº \n","5. Use the pandas library to discard columns you won't use for prediction.\n","\n","In this dataset, some categorical variables have too many modalities, we will have to think about throwing them away: typically, for a dataset that is less than 1000 lines long, we will tend to reject categorical variables that have more than 15-20 possible values. So pay attention to the number of unique values in each column, to decide which ones you will keep."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"fx_InvsWLGPn","outputId":"30725172-42e1-43d7-a499-5892d7ac12a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dropping useless columns...\n","...Done.\n","   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n","0         0       3    male  22.0      1      0   7.2500        S\n","1         1       1  female  38.0      1      0  71.2833        C\n","2         1       3  female  26.0      0      0   7.9250        S\n","3         1       1  female  35.0      1      0  53.1000        S\n","4         0       3    male  35.0      0      0   8.0500        S\n"]}],"source":["# Drop useless columns / columns with too many missing values\n","useless_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n","\n","print(\"Dropping useless columns...\")\n","dataset = dataset.drop(useless_cols, axis=1)\n","print(\"...Done.\")\n","print(dataset.head())"]},{"cell_type":"markdown","metadata":{"id":"SsyYYZcPLGPo"},"source":["6. Separate the target variable (Y) from the explanatory variables (X)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"MAqLJnZhLGPo","outputId":"1cb3b493-f959-4161-d4fe-10ecb5be3e3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Separating labels from features...\n","...Done.\n","0    0\n","1    1\n","2    1\n","3    1\n","4    0\n","Name: Survived, dtype: int64\n","\n","   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n","0       3    male  22.0      1      0   7.2500        S\n","1       1  female  38.0      1      0  71.2833        C\n","2       3  female  26.0      0      0   7.9250        S\n","3       1  female  35.0      1      0  53.1000        S\n","4       3    male  35.0      0      0   8.0500        S\n","\n"]}],"source":["# Separate target variable Y from features X\n","target_name = 'Survived'\n","\n","print(\"Separating labels from features...\")\n","Y = dataset.loc[:,target_name]\n","X = dataset.loc[:,[c for c in dataset.columns if c!=target_name]] # Keeping all columns\n","print(\"...Done.\")\n","print(Y.head())\n","print()\n","print(X.head())\n","print()\n"]},{"cell_type":"markdown","metadata":{"id":"0uW_RsYLLGPp"},"source":["7. Convert pandas objects to numpy objects for use with scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"02MGj_IYLGPp","outputId":"81277831-b81e-46cd-96c1-730a7c5c9570"},"outputs":[{"name":"stdout","output_type":"stream","text":["Convert pandas DataFrames to numpy arrays...\n","...Done\n","[[3 'male' 22.0 1 0 7.25 'S']\n"," [1 'female' 38.0 1 0 71.2833 'C']\n"," [3 'female' 26.0 0 0 7.925 'S']\n"," [1 'female' 35.0 1 0 53.1 'S']\n"," [3 'male' 35.0 0 0 8.05 'S']]\n","\n","[0, 1, 1, 1, 0]\n"]}],"source":["# Convert pandas DataFrames to numpy arrays before using scikit-learn\n","print(\"Convert pandas DataFrames to numpy arrays...\")\n","X = X.values\n","Y = Y.tolist()\n","print(\"...Done\")\n","print(X[0:5,:])\n","print()\n","print(Y[0:5])"]},{"cell_type":"markdown","metadata":{"id":"e8zLLme3LGPp"},"source":["## Preprocessing - scikit-learn part üî¨üî¨\n","8. Separate your data to create a train set and a test set, the latter should represent 15% of the available data."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"CPz6x_bfLGPq","outputId":"e945c578-680f-421b-8124-a86360250b42"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dividing into train and test sets...\n","...Done.\n","\n"]}],"source":["from sklearn.model_selection import train_test_split\n","print(\"Dividing into train and test sets...\")\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=0)\n","print(\"...Done.\")\n","print()"]},{"cell_type":"markdown","metadata":{"id":"GMvdFrFvLGPq"},"source":["9. Create the preprocessing pipeline for numeric columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5eyj3PELGPq"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"dsezAW8FLGPq","outputId":"8f118996-59d8-411f-8f9f-ed54418aff5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 'male' 64.0 0 0 26.0 'S']\n"," [3 'male' 21.0 0 0 8.05 'S']\n"," [3 'male' nan 1 0 7.75 'Q']\n"," [3 'female' 40.0 1 0 9.475 'S']\n"," [2 'male' 44.0 1 0 26.0 'S']]\n"]}],"source":["print(X_train[0:5,:]) # see where the numeric columns are in X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThA9jVytLGPr"},"outputs":[],"source":["# Create pipeline for numeric features\n","numeric_features = [0, 2, 3, 4, 5] # Positions of numeric columns in X_train/X_test\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n","    ('scaler', StandardScaler())\n","])"]},{"cell_type":"markdown","metadata":{"id":"YtPSm-tfLGPr"},"source":["10. Create the preprocessing pipeline for category columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4K4ABH8LGPr"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"biwgwjpqLGPr"},"outputs":[],"source":["# Create pipeline for categorical features\n","categorical_features = [1, 6] # Positions of categorical columns in X_train/X_test\n","categorical_transformer = Pipeline(\n","    steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n","    ('encoder', OneHotEncoder(drop='first')) # first column will be dropped to avoid creating correlations between features\n","    ])"]},{"cell_type":"markdown","metadata":{"id":"YOkn2Ht8LGPr"},"source":["11. Use the preprocessing pipelines of questions 9 and 10 to transform X_train and X_test\n","\n","Reminder: you need to call `fit_transform()` on X_train and only `transform()` on X_test, to ensure that the latter gets the same transformations as X_train_."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91Xy0AcGLGPs"},"outputs":[],"source":["from sklearn.compose import ColumnTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"FtrW1fKDLGPs","outputId":"6b7a30db-49df-404e-ec59-90739694482a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Performing preprocessings on train set...\n","[[1 'male' 64.0 0 0 26.0 'S']\n"," [3 'male' 21.0 0 0 8.05 'S']\n"," [3 'male' nan 1 0 7.75 'Q']\n"," [3 'female' 40.0 1 0 9.475 'S']\n"," [2 'male' 44.0 1 0 26.0 'S']]\n","...Done.\n","[[-1.60067161e+00  2.61131471e+00 -4.63468368e-01 -4.65997851e-01\n","  -1.09604554e-01  1.00000000e+00  0.00000000e+00  1.00000000e+00]\n"," [ 8.10688409e-01 -6.78358906e-01 -4.63468368e-01 -4.65997851e-01\n","  -4.71133941e-01  1.00000000e+00  0.00000000e+00  1.00000000e+00]\n"," [ 8.10688409e-01 -2.71796941e-16  4.31545801e-01 -4.65997851e-01\n","  -4.77176214e-01  1.00000000e+00  1.00000000e+00  0.00000000e+00]\n"," [ 8.10688409e-01  7.75217807e-01  4.31545801e-01 -4.65997851e-01\n","  -4.42433140e-01  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n"," [-3.94991602e-01  1.08123396e+00  4.31545801e-01 -4.65997851e-01\n","  -1.09604554e-01  1.00000000e+00  0.00000000e+00  1.00000000e+00]]\n","\n","Performing preprocessings on test set...\n","[[3 'male' nan 0 0 14.4583 'C']\n"," [3 'male' nan 0 0 7.55 'S']\n"," [3 'male' 7.0 4 1 29.125 'Q']\n"," [1 'female' nan 1 0 146.5208 'C']\n"," [3 'female' 29.0 0 2 15.2458 'C']]\n","...Done.\n","[[ 8.10688409e-01 -2.71796941e-16 -4.63468368e-01 -4.65997851e-01\n","  -3.42064929e-01  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n"," [ 8.10688409e-01 -2.71796941e-16 -4.63468368e-01 -4.65997851e-01\n","  -4.81204397e-01  1.00000000e+00  0.00000000e+00  1.00000000e+00]\n"," [ 8.10688409e-01 -1.74941543e+00  3.11658831e+00  7.80505235e-01\n","  -4.66642017e-02  1.00000000e+00  1.00000000e+00  0.00000000e+00]\n"," [-1.60067161e+00 -2.71796941e-16  4.31545801e-01 -4.65997851e-01\n","   2.31779438e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n"," [ 8.10688409e-01 -6.63266058e-02 -4.63468368e-01  2.02700832e+00\n","  -3.26203960e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n","\n"]}],"source":["# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","# Preprocessings on train set\n","print(\"Performing preprocessings on train set...\")\n","print(X_train[0:5,:])\n","X_train = preprocessor.fit_transform(X_train)\n","print('...Done.')\n","print(X_train[0:5,:])\n","print()\n","\n","# Preprocessings on test set\n","print(\"Performing preprocessings on test set...\")\n","print(X_test[0:5,:])\n","X_test = preprocessor.transform(X_test) # Don't fit again !!\n","print('...Done.')\n","print(X_test[0:5,:])\n","print()"]},{"cell_type":"markdown","metadata":{"id":"V2eNlVEVLGPs"},"source":["### Training model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7aboXhmLGPs"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"4_OM6me9LGPs","outputId":"aeb54932-903f-4659-84b8-75e2fa3d78ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training model...\n","...Done.\n"]},{"name":"stderr","output_type":"stream","text":["/Users/aureliemutschler/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"]}],"source":["# Train model\n","model = LogisticRegression()\n","\n","print(\"Training model...\")\n","model.fit(X_train, Y_train) # Training is always done on train set !!\n","print(\"...Done.\")"]},{"cell_type":"markdown","metadata":{"id":"3KvVpAO7LGPs"},"source":["### Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"_KQnapqzLGPs","outputId":"d6719f07-fa15-4ee2-d175-05bf7b07436d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions on training set...\n","...Done.\n","[0 0 0 0 0]\n","\n"]}],"source":["# Predictions on training set\n","print(\"Predictions on training set...\")\n","Y_train_pred = model.predict(X_train)\n","print(\"...Done.\")\n","print(Y_train_pred[0:5])\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"TyHD0qXOLGPt","outputId":"4e31cdeb-bbd3-40e8-b601-290d7d240a63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions on test set...\n","...Done.\n","[0 0 0 1 1]\n","\n"]}],"source":["# Predictions on test set\n","print(\"Predictions on test set...\")\n","Y_test_pred = model.predict(X_test)\n","print(\"...Done.\")\n","print(Y_test_pred[0:5])\n","print()"]},{"cell_type":"markdown","metadata":{"id":"wX4SWH7xLGPt"},"source":["### Performances evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p6AjgYbILGPt"},"outputs":[],"source":["from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"Jznz1YT7LGPt","outputId":"5caac28b-165c-43a3-f670-9bc4f822cd1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on training set :  0.808454425363276\n","Accuracy on test set :  0.7985074626865671\n"]}],"source":["# Print scores\n","print(\"Accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n","print(\"Accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))"]},{"cell_type":"markdown","metadata":{"id":"G0lDkxZcLGPt"},"source":["If you get a score close to 0.79 on the test set, it means that you managed to do all the preprocessings with a good methodology! :-)"]},{"cell_type":"markdown","metadata":{"id":"nbEQaMD4LGPt"},"source":["# Optional - Advanced Solution üßôüßô\n","\n","This solution is perfectly valid and we applied the rules from the lectures strictly. Now that we are more comfortable with Preprocessing with python, let's take a step back and see what we could have done differently by digging into the interpretation of the variables a little deeper.\n","\n","1. Load the data again"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTvZLJFMLGPu","outputId":"24602e5f-0528-4d63-9f50-ef5c1e1edc4e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["dataset = pd.read_csv('train.csv')\n","\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"6KapRzhxLGPu"},"source":["\n","The exploration of the above data makes it possible to know which pre-processing will be necessary:\n","\n","**A. Preprocessing to be planned with pandas**\n","\n","**Unnecessary columns for prediction, to be thrown away** :\n","- _PassengerId_ and _Name_ are passenger identifiers, we won't use them for prediction (these columns don't contain any information)\n","\n","### **- As it is true that _Name_ cannot be used as such for prediction, it contains valuable information on the socio-economic background of the passenger in the form of their title. We will try and extract a _Title_ variable from the variable _Name_**\n","\n","- _Ticket_ and _Cabin_ have too many different modalities, they might not be very useful and if we had to pass them in OneHotEncoding, they would make the number of columns explode in relation to the number of rows.\n","\n","### **- _Ticket_ and _Cabin_ do have way too many modalities in order to be useful for prediction, however, the _Cabin_ variable can easily be used after a slight transformation : let's create a new variable _HasCabin_ which is equal to 1 when the passenger has a cabin number and 0 otherwise.**\n","\n","**Columns with too many missing values, to be discarded** : Cabin\n","\n","\n","**Target variable/target (Y) that we will try to predict, to separate from the others** : Survived\n","\n","**------------**\n","\n","**B. Preprocessings to be planned with scikit-learn**.\n","\n","**Explanatory variables (X)**\n","We need to identify which columns contain categorical variables and which columns contain numerical variables, as they will be treated differently.\n","\n","- Categorical variables : Sex, Embarked, HasCabin, Title\n","- Numerical variables : Class, Age, Bbsp, Parch, Fare.\n","\n","In this dataset, we have both types of variables. It will thus be necessary to plan to create a numeric_transformer (which will call the StandardScaler class) and a categorical_transformer (which will call the OneHotEncoder class). Moreover, as we observe missing values in the _Age_ and _Embarked_ columns, we will have to plan to call the SimpleImputer class to handle the missing values. \n","\n","**Target variable Y**\n","Here, the target variable Y is categorical (survival vs. death) but we notice that it is already encoded in numbers (1 vs. 0). It will therefore not be necessary to go through a label encoding step."]},{"cell_type":"markdown","metadata":{"id":"i3MNRI5FLGPu"},"source":["## Preprocessing - pandas part ##\n","2. Create a column _HasCabin_ in the dataset that is constant equal to 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGtaCj2zLGPu","outputId":"c5d2fb31-a6ab-42f2-c761-dfc7d96a5679"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","      <th>HasCabin</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  HasCabin  \n","0      0         A/5 21171   7.2500   NaN        S         1  \n","1      0          PC 17599  71.2833   C85        C         1  \n","2      0  STON/O2. 3101282   7.9250   NaN        S         1  \n","3      0            113803  53.1000  C123        S         1  \n","4      0            373450   8.0500   NaN        S         1  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["dataset[\"HasCabin\"] = 1\n","\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"UYJU7LpILGPu"},"source":["3. Using a mask, change the value of the variable _HasCabin_ to 0 wherever Cabin is missing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WSUUHNGLGPu","outputId":"e868b509-caa0-424c-e3d5-cf9a6be4cee6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","      <th>HasCabin</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  HasCabin  \n","0      0         A/5 21171   7.2500   NaN        S         0  \n","1      0          PC 17599  71.2833   C85        C         1  \n","2      0  STON/O2. 3101282   7.9250   NaN        S         0  \n","3      0            113803  53.1000  C123        S         1  \n","4      0            373450   8.0500   NaN        S         0  "]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["dataset.loc[dataset[\"Cabin\"].isnull(),\"HasCabin\"] = 0\n","\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"Qu1Tw_KVLGPu"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"kdWL3-NKLGPv"},"source":["4. Import the regular expression library re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71oWUkmoLGPv"},"outputs":[],"source":["import re"]},{"cell_type":"markdown","metadata":{"id":"4Lo2Mk6DLGPv"},"source":["5. Create a column _Title_ that only contains the title extracted from the _Name_ variable. \n","_Tips_ : You can first create a fuction that extracts the title from a single string and then use the _apply_ method to the variable _Name_."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlqN521vLGPv","outputId":"878c9eef-ac09-474c-c9dd-850958dd5410"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","      <th>HasCabin</th>\n","      <th>Title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>Mr</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","      <td>1</td>\n","      <td>Mrs</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>Miss</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","      <td>1</td>\n","      <td>Mrs</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>Mr</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  HasCabin Title  \n","0      0         A/5 21171   7.2500   NaN        S         0    Mr  \n","1      0          PC 17599  71.2833   C85        C         1   Mrs  \n","2      0  STON/O2. 3101282   7.9250   NaN        S         0  Miss  \n","3      0            113803  53.1000  C123        S         1   Mrs  \n","4      0            373450   8.0500   NaN        S         0    Mr  "]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Define function to extract titles from passenger names\n","def get_title(name):\n","    title_search = re.search(' ([A-Za-z]+)\\.', name)\n","    # If the title exists, extract and return it.\n","    if title_search:\n","        return title_search.group(1)\n","    return \"\"\n","\n","# Create a new feature Title, containing the titles of passenger names\n","dataset['Title'] = dataset['Name'].apply(get_title)\n","\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"tIf_effZLGPv"},"source":["6. Display all the possible values and number of instances of each of these values in your dataset for the new _Title_ variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7oKd03tXLGPv","outputId":"c3f1377a-816b-41cd-a837-a8cf2708a9b0"},"outputs":[{"data":{"text/plain":["Mr          517\n","Miss        182\n","Mrs         125\n","Master       40\n","Dr            7\n","Rev           6\n","Col           2\n","Major         2\n","Mlle          2\n","Capt          1\n","Sir           1\n","Jonkheer      1\n","Countess      1\n","Lady          1\n","Ms            1\n","Don           1\n","Mme           1\n","Name: Title, dtype: int64"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["dataset[\"Title\"].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"xs-OK9-cLGPv"},"source":["7. Some of these values represent only very few instances, and other values seem to represent the similar categories of people. Bring the similar categories under one name, and create a new category called _Rare_ that will represent all the underrepresented modalities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrWXO5lkLGPv","outputId":"82db674d-ec86-4e88-b80a-f3895fa4afde"},"outputs":[{"data":{"text/plain":["Mr        517\n","Miss      185\n","Mrs       126\n","Master     40\n","Rare       23\n","Name: Title, dtype: int64"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# Group all non-common titles into one single grouping \"Rare\"\n","dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n","\n","# Group all synonimous titles under one category\n","dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n","dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n","dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","\n","dataset['Title'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"BdDD897aLGPw"},"source":["8. Now that we are done squeezing some extra information out of our variables, let's reproduce all the subsequent steps from the first solution and let's compare our models' performances."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"vxw-cv02LGPw","outputId":"b43ac2c1-9bd7-47bb-9425-7b5f5b7c195a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dropping useless columns...\n","...Done.\n","   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked  HasCabin  \\\n","0         0       3    male  22.0      1      0   7.2500        S         0   \n","1         1       1  female  38.0      1      0  71.2833        C         1   \n","2         1       3  female  26.0      0      0   7.9250        S         0   \n","3         1       1  female  35.0      1      0  53.1000        S         1   \n","4         0       3    male  35.0      0      0   8.0500        S         0   \n","\n","  Title  \n","0    Mr  \n","1   Mrs  \n","2  Miss  \n","3   Mrs  \n","4    Mr  \n"]}],"source":["# Drop useless columns / columns with too many missing values\n","useless_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n","\n","print(\"Dropping useless columns...\")\n","dataset = dataset.drop(useless_cols, axis=1)\n","print(\"...Done.\")\n","print(dataset.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"r8er35RNLGPw","outputId":"cba07b65-7997-4e48-a782-ddbd0667fa9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Separating labels from features...\n","...Done.\n","0    0\n","1    1\n","2    1\n","3    1\n","4    0\n","Name: Survived, dtype: int64\n","\n","   Pclass     Sex   Age  SibSp  Parch     Fare Embarked  HasCabin Title\n","0       3    male  22.0      1      0   7.2500        S         0    Mr\n","1       1  female  38.0      1      0  71.2833        C         1   Mrs\n","2       3  female  26.0      0      0   7.9250        S         0  Miss\n","3       1  female  35.0      1      0  53.1000        S         1   Mrs\n","4       3    male  35.0      0      0   8.0500        S         0    Mr\n","\n"]}],"source":["# Separate target variable Y from features X\n","target_name = 'Survived'\n","\n","print(\"Separating labels from features...\")\n","Y = dataset.loc[:,target_name]\n","X = dataset.loc[:,[c for c in dataset.columns if c!=target_name]] # Keeping all columns\n","print(\"...Done.\")\n","print(Y.head())\n","print()\n","print(X.head())\n","print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"XO2L1vhRLGPw","outputId":"1e66ad62-e37c-4ae3-e3ec-599277b0dfc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Convert pandas DataFrames to numpy arrays...\n","...Done\n","[[3 'male' 22.0 1 0 7.25 'S' 0 'Mr']\n"," [1 'female' 38.0 1 0 71.2833 'C' 1 'Mrs']\n"," [3 'female' 26.0 0 0 7.925 'S' 0 'Miss']\n"," [1 'female' 35.0 1 0 53.1 'S' 1 'Mrs']\n"," [3 'male' 35.0 0 0 8.05 'S' 0 'Mr']]\n","\n","[0, 1, 1, 1, 0]\n"]}],"source":["# Convert pandas DataFrames to numpy arrays before using scikit-learn\n","print(\"Convert pandas DataFrames to numpy arrays...\")\n","X = X.values\n","Y = Y.tolist()\n","print(\"...Done\")\n","print(X[0:5,:])\n","print()\n","print(Y[0:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Cwu-yPJ-LGPw","outputId":"997f1f61-954a-45a4-ba60-a0aa2ae1c591"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dividing into train and test sets...\n","...Done.\n","\n"]}],"source":["from sklearn.model_selection import train_test_split\n","print(\"Dividing into train and test sets...\")\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=0)\n","print(\"...Done.\")\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVmeJe84LGPw"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_I1eXFILGPw"},"outputs":[],"source":["# Create pipeline for numeric features\n","numeric_features = [0, 2, 3, 4, 5, 7] # Positions of numeric columns in X_train/X_test\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n","    ('scaler', StandardScaler())\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDpUzHrYLGPx"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2FxYpoHLGPx"},"outputs":[],"source":["# Create pipeline for categorical features\n","categorical_features = [1, 6, 8] # Positions of categorical columns in X_train/X_test\n","categorical_transformer = Pipeline(\n","    steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n","    ('encoder', OneHotEncoder(drop='first')) # first column will be dropped to avoid creating correlations between features\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kG3EkilhLGPx"},"outputs":[],"source":["from sklearn.compose import ColumnTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"6kyMGXWDLGPx","outputId":"185490d3-bcf6-4630-b851-350ddcf4074d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Performing preprocessings on train set...\n","[[1 'male' 64.0 0 0 26.0 'S' 0 'Mr']\n"," [3 'male' 21.0 0 0 8.05 'S' 0 'Mr']\n"," [3 'male' nan 1 0 7.75 'Q' 0 'Mr']\n"," [3 'female' 40.0 1 0 9.475 'S' 0 'Mrs']\n"," [2 'male' 44.0 1 0 26.0 'S' 0 'Mr']]\n","...Done.\n","[[-1.60067161e+00  2.61131471e+00 -4.63468368e-01 -4.65997851e-01\n","  -1.09604554e-01 -5.36110964e-01  1.00000000e+00  0.00000000e+00\n","   1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n","   0.00000000e+00]\n"," [ 8.10688409e-01 -6.78358906e-01 -4.63468368e-01 -4.65997851e-01\n","  -4.71133941e-01 -5.36110964e-01  1.00000000e+00  0.00000000e+00\n","   1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n","   0.00000000e+00]\n"," [ 8.10688409e-01 -2.71796941e-16  4.31545801e-01 -4.65997851e-01\n","  -4.77176214e-01 -5.36110964e-01  1.00000000e+00  1.00000000e+00\n","   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n","   0.00000000e+00]\n"," [ 8.10688409e-01  7.75217807e-01  4.31545801e-01 -4.65997851e-01\n","  -4.42433140e-01 -5.36110964e-01  0.00000000e+00  0.00000000e+00\n","   1.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n","   0.00000000e+00]\n"," [-3.94991602e-01  1.08123396e+00  4.31545801e-01 -4.65997851e-01\n","  -1.09604554e-01 -5.36110964e-01  1.00000000e+00  0.00000000e+00\n","   1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n","   0.00000000e+00]]\n","\n","Performing preprocessings on test set...\n","[[3 'male' nan 0 0 14.4583 'C' 0 'Mr']\n"," [3 'male' nan 0 0 7.55 'S' 0 'Mr']\n"," [3 'male' 7.0 4 1 29.125 'Q' 0 'Master']\n"," [1 'female' nan 1 0 146.5208 'C' 1 'Mrs']\n"," [3 'female' 29.0 0 2 15.2458 'C' 0 'Mrs']]\n","...Done.\n","[[ 8.10688409e-01 -2.71796941e-16 -4.63468368e-01 -4.65997851e-01\n","  -3.42064929e-01 -5.36110964e-01  1.00000000e+00  0.00000000e+00\n","   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n","   0.00000000e+00]\n"," [ 8.10688409e-01 -2.71796941e-16 -4.63468368e-01 -4.65997851e-01\n","  -4.81204397e-01 -5.36110964e-01  1.00000000e+00  0.00000000e+00\n","   1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n","   0.00000000e+00]\n"," [ 8.10688409e-01 -1.74941543e+00  3.11658831e+00  7.80505235e-01\n","  -4.66642017e-02 -5.36110964e-01  1.00000000e+00  1.00000000e+00\n","   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","   0.00000000e+00]\n"," [-1.60067161e+00 -2.71796941e-16  4.31545801e-01 -4.65997851e-01\n","   2.31779438e+00  1.86528549e+00  0.00000000e+00  0.00000000e+00\n","   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n","   0.00000000e+00]\n"," [ 8.10688409e-01 -6.63266058e-02 -4.63468368e-01  2.02700832e+00\n","  -3.26203960e-01 -5.36110964e-01  0.00000000e+00  0.00000000e+00\n","   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n","   0.00000000e+00]]\n","\n"]}],"source":["# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","# Preprocessings on train set\n","print(\"Performing preprocessings on train set...\")\n","print(X_train[0:5,:])\n","X_train = preprocessor.fit_transform(X_train)\n","print('...Done.')\n","print(X_train[0:5,:])\n","print()\n","\n","# Preprocessings on test set\n","print(\"Performing preprocessings on test set...\")\n","print(X_test[0:5,:])\n","X_test = preprocessor.transform(X_test) # Don't fit again !!\n","print('...Done.')\n","print(X_test[0:5,:])\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"bUekZ830LGPy","outputId":"14c4b13e-ca69-4ba1-e5b5-27f9e9815cc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training model...\n","...Done.\n"]},{"name":"stderr","output_type":"stream","text":["/Users/aureliemutschler/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"]}],"source":["# Train model\n","model = LogisticRegression()\n","\n","print(\"Training model...\")\n","model.fit(X_train, Y_train) # Training is always done on train set !!\n","print(\"...Done.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"3dHbYVC0LGPy","outputId":"25929ada-b2ac-4155-ea48-c7dd67aa4290"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions on training set...\n","...Done.\n","[0 0 0 1 0]\n","\n"]}],"source":["# Predictions on training set\n","print(\"Predictions on training set...\")\n","Y_train_pred = model.predict(X_train)\n","print(\"...Done.\")\n","print(Y_train_pred[0:5])\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"6rjh74w5LGPy","outputId":"6e64c066-f677-4213-de23-0f91245779e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions on test set...\n","...Done.\n","[0 0 0 1 1]\n","\n"]}],"source":["# Predictions on test set\n","print(\"Predictions on test set...\")\n","Y_test_pred = model.predict(X_test)\n","print(\"...Done.\")\n","print(Y_test_pred[0:5])\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"lHCJPF5ZLGPz","outputId":"b3b6502a-b4d8-43b3-cc80-68fad7c90e9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on training set :  0.8282694848084544\n","Accuracy on test set :  0.8208955223880597\n"]}],"source":["# Print scores\n","print(\"Accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n","print(\"Accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))"]},{"cell_type":"markdown","metadata":{"id":"OZiruUVpLGPz"},"source":["Tada!\n","This example shows that by adding a little additional information to a model, it is possible to create a significant impact on the performances of the predictive model. Knowing and applying the preprocessing guidelines is great but always remember to check for two important things before you proceed :\n","\n","* If a variables containes missing values, ask yourself why this value is missing and whether you could use it as information to feed the model with. In the above example, the fact that a passenger does not have a cabin number simply means that they have no cabin. It is very common for missing values to contain hidden meaning, completely random missing values (caused by a bug or other unpredictable causes) are very rare.\n","\n","* When a non-numerical variable is not usable as is, always ask yourself whether you could still extract some information from it. Here the _Name_ variable cannot be used, however it mentions the passenger's title which can be useful information."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"01-Preprocessing_Titanic_solutions.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}